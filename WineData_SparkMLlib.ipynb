{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Packages: spark.mllib vs. spark.ml**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark 2.x goes further with spark.ml\n",
    "spark.ml execution is much faster between 10 and 100 times than the previous.\n",
    "*Easy of hyperparameter tuning.*\n",
    "*Both libraries offer powerful support, however, the speed of execution in spark 2 is clearly higher*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDDs and Spark 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are still the fundamental building blocks of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All operations in Spark are performed on in-memory objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RDD is a **collection** of entities\n",
    "- rows, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\r\n",
      "      ____              __\r\n",
      "     / __/__  ___ _____/ /__\r\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.3\r\n",
      "      /_/\r\n",
      "                        \r\n",
      "Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_211\r\n",
      "Branch \r\n",
      "Compiled by user  on 2019-02-04T13:00:46Z\r\n",
      "Revision \r\n",
      "Url \r\n",
      "Type --help for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.mllib      vs.     Spark.ml\n",
    "- Older***************** - Newer\n",
    "- RDDs                   - DataFrames(faster!)\n",
    "- more functionality     - Functionality catching up\n",
    "- ETL hard - no pipeline - Support for ML pipelines\n",
    "- Hyperparameter.tuning.hard - Tools for hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Trees model for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement classification using decision trees in spark.mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data in the CSV as well as the LIBSVM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the sparkcontext to a variable \"rawData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = sc.textFile(\"datasets/wine.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065',\n",
       " '1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050',\n",
       " '1,13.16,2.36,2.67,18.6,101,2.8,3.24,.3,2.81,5.68,1.03,3.17,1185',\n",
       " '1,14.37,1.95,2.5,16.8,113,3.85,3.49,.24,2.18,7.8,.86,3.45,1480',\n",
       " '1,13.24,2.59,2.87,21,118,2.8,2.69,.39,1.82,4.32,1.04,2.93,735']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.take(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the data in a form to be fed into a decision tree learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(',')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the function to perform a map operation on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData = rawData.map(parsePoint)\n",
    "parsedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [14.23,1.71,2.43,15.6,127.0,2.8,3.06,0.28,2.29,5.64,1.04,3.92,1065.0]),\n",
       " LabeledPoint(1.0, [13.2,1.78,2.14,11.2,100.0,2.65,2.76,0.26,1.28,4.38,1.05,3.4,1050.0]),\n",
       " LabeledPoint(1.0, [13.16,2.36,2.67,18.6,101.0,2.8,3.24,0.3,2.81,5.68,1.03,3.17,1185.0]),\n",
       " LabeledPoint(1.0, [14.37,1.95,2.5,16.8,113.0,3.85,3.49,0.24,2.18,7.8,0.86,3.45,1480.0]),\n",
       " LabeledPoint(1.0, [13.24,2.59,2.87,21.0,118.0,2.8,2.69,0.39,1.82,4.32,1.04,2.93,735.0])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the data for training and test 70% and 30% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = parsedData.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [14.23,1.71,2.43,15.6,127.0,2.8,3.06,0.28,2.29,5.64,1.04,3.92,1065.0]),\n",
       " LabeledPoint(1.0, [13.16,2.36,2.67,18.6,101.0,2.8,3.24,0.3,2.81,5.68,1.03,3.17,1185.0]),\n",
       " LabeledPoint(1.0, [14.37,1.95,2.5,16.8,113.0,3.85,3.49,0.24,2.18,7.8,0.86,3.45,1480.0]),\n",
       " LabeledPoint(1.0, [13.24,2.59,2.87,21.0,118.0,2.8,2.69,0.39,1.82,4.32,1.04,2.93,735.0]),\n",
       " LabeledPoint(1.0, [14.2,1.76,2.45,15.2,112.0,3.27,3.39,0.34,1.97,6.75,1.05,2.85,1450.0])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the built-in machine learning decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainingData,\n",
    "                                    numClasses=4,\n",
    "                                    categoricalFeaturesInfo={},\n",
    "                                    impurity='gini',\n",
    "                                    maxDepth=3,\n",
    "                                    maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the code and use it for predictions using \"predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 3.0, 1.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare whether the predictions are good or bad by comparing them with the actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 3.0), (1.0, 1.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "labelsAndPredictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our logic to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  0.7924528301886793\n"
     ]
    }
   ],
   "source": [
    "testAcc = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] == lp[1]).count() / float(testData.count())\n",
    "\n",
    "print('Test Accuracy = ', testAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating how the model performs on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics(labelsAndPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the built-in accuracy measure available in MultiClassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7924528301886793"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another evaluation method \"fMeasure()\" can be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7924528301886793"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.fMeasure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual score can be extracted as well for every label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"confusionMatrix\" is a row column plot of actual labels vs. prediction labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 3, [12.0, 0.0, 2.0, 0.0, 18.0, 1.0, 0.0, 8.0, 12.0], 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the actual machine learning model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 15 nodes\n",
      "  If (feature 12 <= 760.0)\n",
      "   If (feature 9 <= 4.85)\n",
      "    If (feature 9 <= 4.300000000000001)\n",
      "     Predict: 2.0\n",
      "    Else (feature 9 > 4.300000000000001)\n",
      "     Predict: 2.0\n",
      "   Else (feature 9 > 4.85)\n",
      "    If (feature 1 <= 2.0700000000000003)\n",
      "     Predict: 2.0\n",
      "    Else (feature 1 > 2.0700000000000003)\n",
      "     Predict: 3.0\n",
      "  Else (feature 12 > 760.0)\n",
      "   If (feature 6 <= 2.2800000000000002)\n",
      "    If (feature 1 <= 1.525)\n",
      "     Predict: 2.0\n",
      "    Else (feature 1 > 1.525)\n",
      "     Predict: 3.0\n",
      "   Else (feature 6 > 2.2800000000000002)\n",
      "    If (feature 3 <= 26.75)\n",
      "     Predict: 1.0\n",
      "    Else (feature 3 > 26.75)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with the LIBSVM Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvmData = MLUtils.loadLibSVMFile(sc, 'datasets/wine.scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, (13,[0,1,2,3,4,5,6,7,8,9,10,11,12],[0.68421,-0.616601,0.144385,-0.484536,0.23913,0.255172,0.147679,-0.433962,0.18612,-0.255973,-0.089431,0.941392,0.122682])),\n",
       " LabeledPoint(1.0, (13,[0,1,2,3,4,5,6,7,8,9,10,11,12],[0.142105,-0.588933,-0.165775,-0.938144,-0.347826,0.151724,0.0210971,-0.509434,-0.451104,-0.47099,-0.0731708,0.56044,0.101284])),\n",
       " LabeledPoint(1.0, (13,[0,1,2,3,4,5,6,7,8,9,10,11,12],[0.121053,-0.359684,0.40107,-0.175258,-0.326087,0.255172,0.223629,-0.358491,0.514196,-0.249147,-0.105691,0.391941,0.293866])),\n",
       " LabeledPoint(1.0, (13,[0,1,2,3,4,5,6,7,8,9,10,11,12],[0.757895,-0.521739,0.219251,-0.360825,-0.0652174,0.97931,0.329114,-0.584906,0.116719,0.112628,-0.382114,0.59707,0.714693])),\n",
       " LabeledPoint(1.0, (13,[0,1,2,3,4,5,6,7,8,9,10,11,12],[0.163158,-0.268775,0.614973,0.0721649,0.0434783,0.255172,-0.00843878,-0.018868,-0.11041,-0.481229,-0.089431,0.216117,-0.348074]))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libsvmData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = libsvmData.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvmModel = DecisionTree.trainClassifier(trainingData,\n",
    "                                          numClasses=4,\n",
    "                                          categoricalFeaturesInfo={},\n",
    "                                          impurity='gini',\n",
    "                                          maxDepth=5,\n",
    "                                          maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = libsvmModel.predict(testData.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0), (1.0, 1.0)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "labelsAndPredictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(labelsAndPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.,  1.,  0.],\n",
       "       [ 0., 11.,  1.],\n",
       "       [ 0.,  1., 19.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 15 nodes\n",
      "  If (feature 12 <= 760.0)\n",
      "   If (feature 9 <= 4.85)\n",
      "    If (feature 9 <= 4.300000000000001)\n",
      "     Predict: 2.0\n",
      "    Else (feature 9 > 4.300000000000001)\n",
      "     Predict: 2.0\n",
      "   Else (feature 9 > 4.85)\n",
      "    If (feature 1 <= 2.0700000000000003)\n",
      "     Predict: 2.0\n",
      "    Else (feature 1 > 2.0700000000000003)\n",
      "     Predict: 3.0\n",
      "  Else (feature 12 > 760.0)\n",
      "   If (feature 6 <= 2.2800000000000002)\n",
      "    If (feature 1 <= 1.525)\n",
      "     Predict: 2.0\n",
      "    Else (feature 1 > 1.525)\n",
      "     Predict: 3.0\n",
      "   Else (feature 6 > 2.2800000000000002)\n",
      "    If (feature 3 <= 26.75)\n",
      "     Predict: 1.0\n",
      "    Else (feature 3 > 26.75)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPARK.ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame\n",
    "\n",
    "Use to represent the ML dataset for training\n",
    "\n",
    "Different columns for features, labels, predictions\n",
    "\n",
    "Transformers\n",
    "\n",
    "Algorithm to convert one DataFrame to another\n",
    "\n",
    "DataFrame with features -> DataFrame with predictions\n",
    "\n",
    "Estimator\n",
    "\n",
    "An algorithm that fits on a DataFrame to produce a Transformer\n",
    "\n",
    "An ML algorithm -> trains on input data -> produces a model\n",
    "\n",
    "Pipeline\n",
    "\n",
    "Chains Estimators and Transformers to form a machine learning workflow\n",
    "\n",
    "Chains a series of operations to be performed on DataFrames\n",
    "\n",
    "Parameter\n",
    "\n",
    "Design settings in ML algorithms that can be tuned\n",
    "\n",
    "Transformers and Estimators have a common API for parameters\n",
    "\n",
    "#Pipeline Stages\n",
    "\n",
    "Estimator Stages-------------------------------------------Transformer Stages\n",
    "\n",
    "DataFrame in Transformer out-------------------------------DataFrame in, DataFrame out\n",
    "\n",
    "Implement a fit() method-----------------------------------Implement a transform() method\n",
    "\n",
    "Obtain trained machine learning model by invoking fit()----Transform features or carry out prediction using transform()\n",
    "\n",
    "    Implement classification using Decision Trees in spark.ml\n",
    "\n",
    "    Evaluate the model using the F1 score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark.ml has:\n",
    "\n",
    "    High level abstractions such as Estimators and Transformations\n",
    "\n",
    "    Chained together in a pipeline i.e. machine learning workflow\n",
    "\n",
    "    Special libraries for feature engineering\n",
    "\n",
    "    Evaluating classifiers using the confusion matrix\n",
    "\n",
    "    Decision trees and random forests for classification\n",
    "\n",
    "    Specialized regression models such as Lasso and Ridge regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName('Predicting the grape variety from wine characteristics')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "rawData = spark.read\\\n",
    "            .format('csv')\\\n",
    "            .option('header', 'false')\\\n",
    "            .load('wine.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='1', _c1='14.23', _c2='1.71', _c3='2.43', _c4='15.6', _c5='127', _c6='2.8', _c7='3.06', _c8='.28', _c9='2.29', _c10='5.64', _c11='1.04', _c12='3.92', _c13='1065'),\n",
       " Row(_c0='1', _c1='13.2', _c2='1.78', _c3='2.14', _c4='11.2', _c5='100', _c6='2.65', _c7='2.76', _c8='.26', _c9='1.28', _c10='4.38', _c11='1.05', _c12='3.4', _c13='1050'),\n",
       " Row(_c0='1', _c1='13.16', _c2='2.36', _c3='2.67', _c4='18.6', _c5='101', _c6='2.8', _c7='3.24', _c8='.3', _c9='2.81', _c10='5.68', _c11='1.03', _c12='3.17', _c13='1185'),\n",
       " Row(_c0='1', _c1='14.37', _c2='1.95', _c3='2.5', _c4='16.8', _c5='113', _c6='3.85', _c7='3.49', _c8='.24', _c9='2.18', _c10='7.8', _c11='.86', _c12='3.45', _c13='1480'),\n",
       " Row(_c0='1', _c1='13.24', _c2='2.59', _c3='2.87', _c4='21', _c5='118', _c6='2.8', _c7='2.69', _c8='.39', _c9='1.82', _c10='4.32', _c11='1.04', _c12='2.93', _c13='735')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rawData.toDF('Label',\n",
    "                'Alcohol',\n",
    "                'MalicAcid',\n",
    "                'Ash',\n",
    "                'AshAlkalinity',\n",
    "                'Magnesium',\n",
    "                'TotalPhenols',\n",
    "                'Flavanoids',\n",
    "                'NonflavanoidPhenols',\n",
    "                'Proanthocyanins',\n",
    "                'ColorIntensity',\n",
    "                'Hue',\n",
    "                'OD',\n",
    "                'Proline'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Label: string, Alcohol: string, MalicAcid: string, Ash: string, AshAlkalinity: string, Magnesium: string, TotalPhenols: string, Flavanoids: string, NonflavanoidPhenols: string, Proanthocyanins: string, ColorIntensity: string, Hue: string, OD: string, Proline: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+----+-------------+---------+------------+----------+-------------------+---------------+--------------+----+----+-------+\n",
      "|Label|Alcohol|MalicAcid| Ash|AshAlkalinity|Magnesium|TotalPhenols|Flavanoids|NonflavanoidPhenols|Proanthocyanins|ColorIntensity| Hue|  OD|Proline|\n",
      "+-----+-------+---------+----+-------------+---------+------------+----------+-------------------+---------------+--------------+----+----+-------+\n",
      "|    1|  14.23|     1.71|2.43|         15.6|      127|         2.8|      3.06|                .28|           2.29|          5.64|1.04|3.92|   1065|\n",
      "|    1|   13.2|     1.78|2.14|         11.2|      100|        2.65|      2.76|                .26|           1.28|          4.38|1.05| 3.4|   1050|\n",
      "|    1|  13.16|     2.36|2.67|         18.6|      101|         2.8|      3.24|                 .3|           2.81|          5.68|1.03|3.17|   1185|\n",
      "|    1|  14.37|     1.95| 2.5|         16.8|      113|        3.85|      3.49|                .24|           2.18|           7.8| .86|3.45|   1480|\n",
      "|    1|  13.24|     2.59|2.87|           21|      118|         2.8|      2.69|                .39|           1.82|          4.32|1.04|2.93|    735|\n",
      "+-----+-------+---------+----+-------------+---------+------------+----------+-------------------+---------------+--------------+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def vectorize(data):\n",
    "    return data.rdd.map(lambda r: [r[0], Vectors.dense(r[1:])]).toDF(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizedData = vectorize(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|[14.23,1.71,2.43,...|\n",
      "|    1|[13.2,1.78,2.14,1...|\n",
      "|    1|[13.16,2.36,2.67,...|\n",
      "|    1|[14.37,1.95,2.5,1...|\n",
      "|    1|[13.24,2.59,2.87,...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizedData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='1', features=DenseVector([14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0])),\n",
       " Row(label='1', features=DenseVector([13.2, 1.78, 2.14, 11.2, 100.0, 2.65, 2.76, 0.26, 1.28, 4.38, 1.05, 3.4, 1050.0])),\n",
       " Row(label='1', features=DenseVector([13.16, 2.36, 2.67, 18.6, 101.0, 2.8, 3.24, 0.3, 2.81, 5.68, 1.03, 3.17, 1185.0])),\n",
       " Row(label='1', features=DenseVector([14.37, 1.95, 2.5, 16.8, 113.0, 3.85, 3.49, 0.24, 2.18, 7.8, 0.86, 3.45, 1480.0])),\n",
       " Row(label='1', features=DenseVector([13.24, 2.59, 2.87, 21.0, 118.0, 2.8, 2.69, 0.39, 1.82, 4.32, 1.04, 2.93, 735.0]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer = StringIndexer(inputCol='label',\n",
    "                                            outputCol='indexedLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='1', features=DenseVector([14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0]), indexedLabel=1.0),\n",
       " Row(label='1', features=DenseVector([13.2, 1.78, 2.14, 11.2, 100.0, 2.65, 2.76, 0.26, 1.28, 4.38, 1.05, 3.4, 1050.0]), indexedLabel=1.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexedData = labelIndexer.fit(vectorizedData).transform(vectorizedData)\n",
    "indexedData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, features: vector, indexedLabel: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    3|\n",
      "|    1|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedData.select('label').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|indexedLabel|\n",
      "+------------+\n",
      "|         0.0|\n",
      "|         1.0|\n",
      "|         2.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedData.select('indexedLabel').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = indexedData.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree Classifier\n",
    "\n",
    "    Specify the features and label columns\n",
    "    maxDepth: The maximum depth of the decision tree\n",
    "    impurity: We use gini instead of entropy. Gini measurement is the probability of a random sample being classified correctly. Entropy is a measure of information (seek to maximize information gain when making a split). Outputs generally don't vary much when either option is chosen, but entropy may take longer to compute as it calculates a logarithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(\n",
    "    labelCol='indexedLabel', \n",
    "    featuresCol='features',\n",
    "    maxDepth=3,\n",
    "    impurity='gini'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traing the model using the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dtree.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Spark ML's MulticlassClassificationEvaluator to evaluate the model**\n",
    "\n",
    "    - Used to evaluate classification models\n",
    "    - It takes a set of labels and predictions as input\n",
    "    - Similar to (but not the same as MulticlassMetrics in MLLib)\n",
    "    - metricName: Can be precision, recall, weightedPrecision, weightedRecall and f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel',\n",
    "                                              predictionCol='prediction', \n",
    "                                              metricName='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the test data using our model to include predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+--------------+-------------+----------+\n",
      "|label|            features|indexedLabel| rawPrediction|  probability|prediction|\n",
      "+-----+--------------------+------------+--------------+-------------+----------+\n",
      "|    1|[13.16,2.36,2.67,...|         1.0|[0.0,44.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    1|[13.2,1.78,2.14,1...|         1.0|[0.0,44.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    1|[13.51,1.8,2.65,1...|         1.0|[0.0,44.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    1|[13.72,1.43,2.5,1...|         1.0|[0.0,44.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    1|[13.75,1.73,2.41,...|         1.0|[0.0,44.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "+-----+--------------------+------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data = model.transform(testData)\n",
    "transformed_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure accuracy of model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 accuracy: 0.9245538720538721\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.getMetricName(), \n",
    "      'accuracy:', \n",
    "      evaluator.evaluate(transformed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
